{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-ogrady/dissertation2/blob/main/c37_train_AMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvOE0hNU7G29"
      },
      "source": [
        "# Part 2 - Training a CNN on word recogntion task\n",
        "\n",
        "Follow up to the processing. Now we train a CNN to recognise the digits spoken in the audio. We use spectrograms and AlexNet so that we can compare with work by Becker et al (https://arxiv.org/abs/1807.03418)\n",
        "\n",
        "**Work flow**\n",
        "1. We set up the CNN by extending the nn.Module class and define the training loop (hyperparameter is learning rate only). We use a validation set to impose early stopping with a patience of 10 epochs\n",
        "2. The data has been been processed so we load it and create a dataset to use with the dataloader\n",
        "3. Train! \n",
        "\n",
        "**Findings**\n",
        "\n",
        "In our processing, we deviated slightly from Becker's procedure. We do here too (eg a different optimiser: Adam, instead of SGD) but they are still comparable. We end up with a model that trains with high (validation) accuracy and is actually abandoned, because I required more 'challenging' tasks in my dissertation. However, it is worth noting that Becker's approach provides a model with 96% and we gain +99% with only 40% of the data they used.\n",
        "\n",
        "**Notes**\n",
        "* Again, training is deliberately abandoned. Here, I was seaking to create a base model to demonstrate that the novel loss function in my dissertation provides an improvement. But if I was gaining close to 100% accuracy on the base, I would be unable to demonstrate much of an improvement!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EkvqdToD7G3A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# random seeds\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "generator1=torch.Generator().manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqOcwvxfBlMa"
      },
      "source": [
        "## Define network and training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create CNN with AlexNet architecture\n",
        "# credit to author Nouman\n",
        "# https://blog.paperspace.com/alexnet-pytorch/\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, input_channels=3,num_classes=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(9216, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rETqo541pkyL"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "def trainer0(n_epochs,lrs,train_dataloader,val_dataloader):\n",
        "    \n",
        "    # maximise available memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # storage\n",
        "    # data dict for storing metrics per lr and epoch\n",
        "    epoch_keys=[\"train_losses\",\"train_accuracies\",\n",
        "                \"val_losses\",\"val_accuracies\",\"time\"]\n",
        "    \n",
        "    data_dict={lr:\n",
        "      {key:[] for key in epoch_keys} for lr in lrs}\n",
        "    \n",
        "    # we will store the model filenames for reference\n",
        "    best_model_strings=[]\n",
        "\n",
        "    # we train a new model for each lr value\n",
        "    for i,lr in enumerate(lrs):\n",
        "        # load model for each loop\n",
        "        model = AlexNet(input_channels=1).to(device)\n",
        "        optimizer = optim.Adam(params = model.parameters(),lr=lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # initialise variables for EarlyStopping\n",
        "        best_loss = float('inf')\n",
        "        best_model_weights = None\n",
        "        patience = 10\n",
        "\n",
        "\n",
        "        print(f\"training for lr = {lr}\")\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            t0 = time.time()\n",
        "            # initialise variables for recording training loss and accuracy\n",
        "            running_loss = 0.0\n",
        "            running_correct = 0\n",
        "            running_total = 0\n",
        "\n",
        "            model.train()\n",
        "            for input,labels in train_dataloader:\n",
        "                desired_labels=labels[0]\n",
        "\n",
        "                input = input.to(device)\n",
        "                desired_labels = desired_labels.to(device)\n",
        "\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(input)\n",
        "                loss = criterion(output,desired_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # training loss and accuracy for this batch\n",
        "                # this is scaled by the batch size and divided back later\n",
        "                running_loss += loss.item() * input.size(0)\n",
        "\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                running_total += desired_labels.size(0)\n",
        "                running_correct += (predicted == desired_labels).sum().item()\n",
        "\n",
        "            # training loss and accuracy for epoch (scaled back)\n",
        "            train_loss = running_loss/len(train_dataloader.dataset)\n",
        "\n",
        "            train_accuracy = 100 * running_correct/running_total\n",
        "\n",
        "            # store training loss and acuracy in our dictionary\n",
        "            epoch_dict=data_dict[lr]\n",
        "            epoch_dict[\"train_losses\"].append(train_loss)\n",
        "            epoch_dict[\"train_accuracies\"].append(train_accuracy)\n",
        "\n",
        "            # initialise variables for recording val loss and accuracy\n",
        "            val_running_loss = 0.0\n",
        "            val_running_correct = 0\n",
        "            val_running_total = 0\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for input,labels in val_dataloader:\n",
        "                    desired_labels=labels[0]\n",
        "                    input = input.to(device)\n",
        "                    desired_labels = desired_labels.to(device)\n",
        "\n",
        "                    output = model(input)\n",
        "\n",
        "                    # validation loss and accuracy for batch\n",
        "                    val_running_loss += criterion(output,desired_labels).item() * desired_labels.size(0)\n",
        "\n",
        "                    _, predicted = torch.max(output.data, 1)\n",
        "                    val_running_total += desired_labels.size(0)\n",
        "                    val_running_correct += (predicted == desired_labels).sum().item()\n",
        "\n",
        "\n",
        "            # validation loss and accuracy for epoch\n",
        "            val_loss = val_running_loss / len(val_dataloader.dataset)\n",
        "\n",
        "            val_accuracy = 100 * val_running_correct / val_running_total\n",
        "\n",
        "            # store\n",
        "            epoch_time=time.time()-t0\n",
        "            epoch_dict[\"val_losses\"].append(val_loss)\n",
        "\n",
        "            epoch_dict[\"val_accuracies\"].append(val_accuracy)\n",
        "            epoch_dict[\"time\"].append(epoch_time)\n",
        "\n",
        "            # print for epoch\n",
        "            if epoch % 5 == 0:\n",
        "                print(f\"epoch: {epoch + 1}, time: {epoch_time:0.2f}\")\n",
        "                print(\"training loss: \",\n",
        "                      f\"{train_loss:0.2f}, accuracy: {train_accuracy:.2f}\")\n",
        "                print(\"validation loss: \",\n",
        "                      f\"{val_loss:0.2f}, accuracy: {val_accuracy:.2f}\")\n",
        "                print()\n",
        "\n",
        "            # EarlyStopping\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                best_model_weights = copy.deepcopy(model.state_dict())\n",
        "                patience = 10\n",
        "            else:\n",
        "                patience -= 1\n",
        "                if patience == 0:\n",
        "                    print(f\"Early stop at epoch {epoch+1}\\n\")\n",
        "                    break\n",
        "\n",
        "        # load and store the best model weights\n",
        "        model.load_state_dict(best_model_weights)\n",
        "        time_stamp=time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        file_path=f\"output/model_base_lr{lr}_{time_stamp}.pth\"\n",
        "        torch.save(model.state_dict(), file_path)\n",
        "        print(f\"saved model as '{file_path}'\")\n",
        "        print()\n",
        "        best_model_strings.append(file_path)\n",
        "\n",
        "    return data_dict, best_model_strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wgpDMG2dF3SE"
      },
      "outputs": [],
      "source": [
        "# test accuracy of a given model\n",
        "# this is not used because we abandon the training\n",
        "def tester(model_string,test_dataloader,target_type=None):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    model = AlexNet(input_channels=1)\n",
        "    model.load_state_dict(torch.load(model_string))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input,target in test_dataloader:\n",
        "            if target_type==\"tuple\":\n",
        "                target=target[0]\n",
        "            input,target = input.to(device),target.to(device)\n",
        "            scores = model(input)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct +=  (predictions ==  target).sum()\n",
        "            num_samples +=  predictions.size(0)\n",
        "\n",
        "    accuracy = float(num_correct)/float(num_samples)*100\n",
        "\n",
        "    print(model_string)\n",
        "    print(f'Got {num_correct}/{num_samples} with accuracy {accuracy:.2f}\\n')\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alJL0b7t7G3B"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD7mxpVPYT6l",
        "outputId": "ea2c8fbe-ffa7-44fb-8c77-6ecc52e0f98f"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "my_data=torch.load('data/AudioMNIST_processed/my_small_data.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cE25r67FYT6l"
      },
      "outputs": [],
      "source": [
        "# we create our dataset class\n",
        "# add another label for additional work in my disseratation\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, my_list):\n",
        "        self.my_list = my_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.my_list[0])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        spec=self.my_list[0][index]\n",
        "        digit=self.my_list[1][index]\n",
        "        gender=self.my_list[3][index]\n",
        "\n",
        "        return spec, (digit,gender)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ddA3ws6YYT6l"
      },
      "outputs": [],
      "source": [
        "amnist_dataset=MyDataset(my_data)\n",
        "\n",
        "# split the data into train/validation/test\n",
        "amnist_train_dataset,amnist_val_dataset,amnist_test_dataset=torch.utils.data.random_split(amnist_dataset,[2/3,1/6,1/6],generator=generator1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unHDI-iwpkyN",
        "outputId": "2d8d35be-9415-4de7-97f9-4196d49a189c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amnist train len 4000\n",
            "amnist val len 1000\n",
            "amnist test len 1000\n",
            "image shape torch.Size([1, 227, 227])\n",
            "labels (tensor(3), tensor(0))\n"
          ]
        }
      ],
      "source": [
        "# double check shapes and size\n",
        "print(f\"amnist train len {len(amnist_train_dataset)}\")\n",
        "print(f\"amnist val len {len(amnist_val_dataset)}\")\n",
        "print(f\"amnist test len {len(amnist_test_dataset)}\")\n",
        "\n",
        "print(f\"image shape {amnist_train_dataset[0][0].shape}\")\n",
        "print(f'labels {amnist_train_dataset[0][1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oPU1heiApkyO"
      },
      "outputs": [],
      "source": [
        "# dataloaders\n",
        "amnist_train_dataloader = DataLoader(amnist_train_dataset, batch_size = 64, shuffle = True,generator = generator1)\n",
        "amnist_val_dataloader = DataLoader(amnist_val_dataset, batch_size = 64, shuffle = True,generator = generator1)\n",
        "amnist_test_dataloader = DataLoader(amnist_test_dataset, batch_size = 64, shuffle = True,generator = generator1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPzSiT2tpkyO"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGupiSDETSlD"
      },
      "source": [
        "### Use GPU if possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3C2FJj-Ukw-",
        "outputId": "9c2042ea-7cb5-41de-f232-ae6d8a848e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Sep  3 10:44:17 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "\n",
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    # show GPU details - this is a colab command\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"\\nUsing {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YplE9Vcd7G3I",
        "outputId": "a0755db5-0d10-4b47-aaa3-8dcce4870c54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 55, 55]          11,712\n",
            "       BatchNorm2d-2           [-1, 96, 55, 55]             192\n",
            "              ReLU-3           [-1, 96, 55, 55]               0\n",
            "         MaxPool2d-4           [-1, 96, 27, 27]               0\n",
            "            Conv2d-5          [-1, 256, 27, 27]         614,656\n",
            "       BatchNorm2d-6          [-1, 256, 27, 27]             512\n",
            "              ReLU-7          [-1, 256, 27, 27]               0\n",
            "         MaxPool2d-8          [-1, 256, 13, 13]               0\n",
            "            Conv2d-9          [-1, 384, 13, 13]         885,120\n",
            "      BatchNorm2d-10          [-1, 384, 13, 13]             768\n",
            "             ReLU-11          [-1, 384, 13, 13]               0\n",
            "           Conv2d-12          [-1, 384, 13, 13]       1,327,488\n",
            "      BatchNorm2d-13          [-1, 384, 13, 13]             768\n",
            "             ReLU-14          [-1, 384, 13, 13]               0\n",
            "           Conv2d-15          [-1, 256, 13, 13]         884,992\n",
            "      BatchNorm2d-16          [-1, 256, 13, 13]             512\n",
            "             ReLU-17          [-1, 256, 13, 13]               0\n",
            "        MaxPool2d-18            [-1, 256, 6, 6]               0\n",
            "          Dropout-19                 [-1, 9216]               0\n",
            "           Linear-20                 [-1, 4096]      37,752,832\n",
            "             ReLU-21                 [-1, 4096]               0\n",
            "          Dropout-22                 [-1, 4096]               0\n",
            "           Linear-23                 [-1, 4096]      16,781,312\n",
            "             ReLU-24                 [-1, 4096]               0\n",
            "           Linear-25                   [-1, 10]          40,970\n",
            "================================================================\n",
            "Total params: 58,301,834\n",
            "Trainable params: 58,301,834\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.20\n",
            "Forward/backward pass size (MB): 16.04\n",
            "Params size (MB): 222.40\n",
            "Estimated Total Size (MB): 238.64\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# show the model architecture and size\n",
        "model0 = AlexNet(input_channels=1).to(device)\n",
        "# overview of model - we enter the input size\n",
        "summary(model0,(1, 227, 227),device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzkfZKz0YT6n"
      },
      "source": [
        "### Training loop\n",
        "Note: we interupt this, because we start getting very high accuracy (which we don't actually want in my dissertation!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RV6_n0RcpkyP"
      },
      "outputs": [],
      "source": [
        "# hyperparams\n",
        "# lrs and n_epochs for all\n",
        "lrs=[0.0001,0.0005,0.001]\n",
        "n_epochs=100\n",
        "\n",
        "# loss\n",
        "base_loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "qEcBs22FpkyP",
        "outputId": "6be9d453-1d5f-4862-a253-d283bfc7d9eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training for lr = 0.0001\n",
            "epoch: 1, time: 6.55\n",
            "training loss:  0.87, accuracy: 70.10\n",
            "validation loss:  0.41, accuracy: 83.40\n",
            "\n",
            "epoch: 6, time: 5.87\n",
            "training loss:  0.05, accuracy: 98.50\n",
            "validation loss:  0.34, accuracy: 91.10\n",
            "\n",
            "epoch: 11, time: 5.79\n",
            "training loss:  0.00, accuracy: 99.97\n",
            "validation loss:  0.01, accuracy: 99.70\n",
            "\n",
            "epoch: 16, time: 5.71\n",
            "training loss:  0.01, accuracy: 99.75\n",
            "validation loss:  0.03, accuracy: 99.20\n",
            "\n",
            "epoch: 21, time: 5.72\n",
            "training loss:  0.01, accuracy: 99.78\n",
            "validation loss:  0.01, accuracy: 99.50\n",
            "\n",
            "epoch: 26, time: 5.96\n",
            "training loss:  0.00, accuracy: 99.92\n",
            "validation loss:  0.02, accuracy: 99.30\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ca3c528313e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel02_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel02_strings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mamnist_train_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mamnist_val_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-189683c9d71a>\u001b[0m in \u001b[0;36mtrainer0\u001b[0;34m(n_epochs, lrs, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0;31m# validation loss and accuracy for batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0mval_running_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdesired_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdesired_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# our output from the training loop is a dictionary and list of file names\n",
        "model02_dict,model02_strings=trainer0(n_epochs,lrs,amnist_train_dataloader,amnist_val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_x7Mmq32-7X",
        "outputId": "c4554959-53a6-4679-a709-d81ad9d9c3f4"
      },
      "outputs": [],
      "source": [
        "# saving\n",
        "time_stamp=time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "torch.save(model02_dict, f'output/model02_dict_{time_stamp}.pt')\n",
        "print(f\"saved dictionary as 'model02_dict_{time_stamp}.pt'\")\n",
        "\n",
        "# reference for loading\n",
        "print(model02_strings)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wqOcwvxfBlMa",
        "AGupiSDETSlD",
        "WzkfZKz0YT6n"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
